# Experiment Tracking

MLTrack enhances MLflow's experiment tracking with a simpler API, automatic logging, and a beautiful UI that makes finding and comparing experiments a joy.

## Enhanced Tracking Features

### Automatic Logging
MLTrack automatically captures more information than vanilla MLflow:

```python
# Before (MLflow)
with mlflow.start_run():
    mlflow.log_param("learning_rate", 0.01)
    mlflow.log_param("batch_size", 32)
    mlflow.log_metric("accuracy", 0.95)
    mlflow.sklearn.log_model(model, "model")

# After (MLTrack)
ml.track()  # That's it! Parameters, metrics, and models logged automatically
```

### What Gets Tracked

<div className="grid grid-cols-1 md:grid-cols-2 gap-4 mt-6">
  <div>
    <h4 className="font-semibold mb-2">Automatically Captured</h4>
    <ul className="space-y-2 text-sm">
      <li>✓ All function parameters</li>
      <li>✓ Model performance metrics</li>
      <li>✓ System metrics (GPU, CPU, memory)</li>
      <li>✓ Training duration and cost</li>
      <li>✓ Git commit and diff</li>
      <li>✓ Python environment</li>
      <li>✓ Dataset statistics</li>
    </ul>
  </div>
  <div>
    <h4 className="font-semibold mb-2">Enhanced Tracking</h4>
    <ul className="space-y-2 text-sm">
      <li>✓ Live metric streaming</li>
      <li>✓ Automatic hyperparam detection</li>
      <li>✓ Smart metric aggregation</li>
      <li>✓ Cost tracking per run</li>
      <li>✓ Team member attribution</li>
      <li>✓ Experiment lineage</li>
      <li>✓ Auto-tagging</li>
    </ul>
  </div>
</div>

## Smart Tracking

MLTrack intelligently detects what to track based on your code:

### Framework Detection
```python
# MLTrack automatically detects and logs framework-specific metrics

# PyTorch
model = torch.nn.Linear(10, 1)
# Logs: model architecture, parameter count, optimizer settings

# Scikit-learn  
model = RandomForestClassifier(n_estimators=100)
# Logs: feature importance, cross-validation scores

# TensorFlow/Keras
model = tf.keras.Sequential([...])
# Logs: layer configuration, callbacks, training history
```

### Hyperparameter Detection
```python
# Any variable that looks like a hyperparameter is logged
learning_rate = 0.001  # Automatically logged
batch_size = 32        # Automatically logged
n_epochs = 100         # Automatically logged

# Even from config files
config = yaml.load("config.yaml")
# All config values automatically logged as parameters
```

## Enhanced UI Features

The MLTrack dashboard provides powerful tools for experiment analysis:

### Visual Comparison
<Alert type="info">
Select multiple runs and compare them side-by-side with interactive charts, parallel coordinates, and diff views.
</Alert>

### Smart Filtering
Find experiments instantly with natural language search:
- `"accuracy > 0.9"` - Find high-performing models
- `"yesterday pytorch"` - Recent PyTorch experiments  
- `"cost < $10"` - Budget-friendly runs
- `"sarah's bert models"` - Team member's work

### Live Monitoring
Watch experiments in real-time:
- Streaming metrics charts
- Resource utilization graphs
- Cost accumulation tracking
- Early stopping suggestions

## Tracking Best Practices

### Use Decorators
```python
@ml.track()
def train_model(learning_rate=0.01, batch_size=32):
    # Your training code
    model = create_model()
    # ... training logic ...
    return model

# Everything is tracked automatically!
model = train_model(learning_rate=0.001)
```

### Track Custom Metrics
```python
# Log custom metrics anytime
ml.log_metric("custom_score", 0.95)
ml.log_metrics({
    "precision": 0.92,
    "recall": 0.88,
    "f1": 0.90
})

# Log over time
for epoch in range(n_epochs):
    ml.log_metric("loss", loss, step=epoch)
```

### Organize with Tags
```python
ml.set_tags({
    "model_type": "transformer",
    "dataset": "customer_churn",
    "priority": "high"
})
```

### Track Artifacts
```python
# Models
ml.log_model(model, "random_forest")

# Plots
ml.log_figure(plt.gcf(), "confusion_matrix.png")

# Data
ml.log_artifact("predictions.csv")
```

## Integration with MLflow

All MLTrack tracking is 100% compatible with MLflow:

```python
import mlflow
import mltrack as ml

# Use MLTrack enhanced tracking
ml.track()

# But you can still use MLflow directly
mlflow.log_param("custom_param", value)

# They work together seamlessly!
```

## Advanced Features

### Experiment Lineage
Track relationships between experiments:
```python
# Continue from a previous run
ml.continue_run("previous_run_id")

# Create experiment variants
ml.create_variant("baseline_run_id", 
    changes={"learning_rate": 0.01})
```

### Automatic Baselines
```python
# Compare against saved baselines
ml.compare_to_baseline("production_model")
```

### Cost Optimization
```python
# Get cost estimates before training
ml.estimate_cost(
    instance_type="p3.2xlarge",
    estimated_hours=2
)
```

## Next Steps

- Learn about [Model Management](/docs/core-concepts/models) in MLTrack
- Explore [Deployment](/docs/core-concepts/deployment) capabilities
- Understand [Cost Tracking](/docs/core-concepts/cost-tracking) features